{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ecc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the nessacary libraries to run the pose estimator\n",
    "\n",
    "#Import mediapipe to be used for the model\n",
    "import mediapipe as mp\n",
    "#Import opencv for rendaring and drawing capabilities\n",
    "import cv2\n",
    "\n",
    "import numpy as np #Handle numpy arrays\n",
    "import pandas as pd #Handle tabular data\n",
    "import os #Handle folder structure\n",
    "import pickle #Save and oad ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4859f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "#Import the model from the binary file\n",
    "with open('rf_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    print(\"Model Loaded\")\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b57ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  PoseLandmark.NOSE\n",
      "1  -  PoseLandmark.LEFT_EYE_INNER\n",
      "2  -  PoseLandmark.LEFT_EYE\n",
      "3  -  PoseLandmark.LEFT_EYE_OUTER\n",
      "4  -  PoseLandmark.RIGHT_EYE_INNER\n",
      "5  -  PoseLandmark.RIGHT_EYE\n",
      "6  -  PoseLandmark.RIGHT_EYE_OUTER\n",
      "7  -  PoseLandmark.LEFT_EAR\n",
      "8  -  PoseLandmark.RIGHT_EAR\n",
      "9  -  PoseLandmark.MOUTH_LEFT\n",
      "10  -  PoseLandmark.MOUTH_RIGHT\n",
      "11  -  PoseLandmark.LEFT_SHOULDER\n",
      "12  -  PoseLandmark.RIGHT_SHOULDER\n",
      "13  -  PoseLandmark.LEFT_ELBOW\n",
      "14  -  PoseLandmark.RIGHT_ELBOW\n",
      "15  -  PoseLandmark.LEFT_WRIST\n",
      "16  -  PoseLandmark.RIGHT_WRIST\n",
      "17  -  PoseLandmark.LEFT_PINKY\n",
      "18  -  PoseLandmark.RIGHT_PINKY\n",
      "19  -  PoseLandmark.LEFT_INDEX\n",
      "20  -  PoseLandmark.RIGHT_INDEX\n",
      "21  -  PoseLandmark.LEFT_THUMB\n",
      "22  -  PoseLandmark.RIGHT_THUMB\n",
      "23  -  PoseLandmark.LEFT_HIP\n",
      "24  -  PoseLandmark.RIGHT_HIP\n",
      "25  -  PoseLandmark.LEFT_KNEE\n",
      "26  -  PoseLandmark.RIGHT_KNEE\n",
      "27  -  PoseLandmark.LEFT_ANKLE\n",
      "28  -  PoseLandmark.RIGHT_ANKLE\n",
      "29  -  PoseLandmark.LEFT_HEEL\n",
      "30  -  PoseLandmark.RIGHT_HEEL\n",
      "31  -  PoseLandmark.LEFT_FOOT_INDEX\n",
      "32  -  PoseLandmark.RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
   "source": [
    "#Display the results of the prediction done by the model\n",
    "draw_helpers = mp.solutions.drawing_utils \n",
    "holistic_model = mp.solutions.holistic\n",
    "\n",
    "#Use hollistics model to list down the landmarks \n",
    "for id, landmark in enumerate(holistic_model.PoseLandmark):\n",
    "    print(id, \" - \", landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b606ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate angle between 3 landmark points\n",
    "def calculate_pose_angle(start_point, mid_point, end_point):\n",
    "    #Convert landmark coords to numpy array\n",
    "#     start_point = np.array(start_point) \n",
    "#     mid_point = np.array(mid_point) \n",
    "#     end_point = np.array(end_point) \n",
    "    \n",
    "    max_angle = 180.0\n",
    "    \n",
    "    #[0] = x, [1] = y, [2] = z\n",
    "    radians = np.arctan2(end_point[1] - mid_point[1], end_point[0] - mid_point[0]) - np.arctan2(start_point[1] - mid_point[1], start_point[0] - mid_point[0])\n",
    "    #Convert to an angle\n",
    "    angle = np.abs(radians * max_angle / np.pi)\n",
    "    \n",
    "    if angle > max_angle:\n",
    "        angle = 360 - angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47a8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "The Rep No  1  was  Correct\n",
      "Average Correct Percentage of the Rep:  0.8531884057971018\n",
      "Average Incorrect Percentage of the Rep:  0.14681159420289852\n",
      "\n",
      "The Rep No  2  was  Correct\n",
      "Average Correct Percentage of the Rep:  0.8501075268817209\n",
      "Average Incorrect Percentage of the Rep:  0.14989247311827972\n",
      "\n",
      "The Rep No  3  was  Correct\n",
      "Average Correct Percentage of the Rep:  0.8505154639175256\n",
      "Average Incorrect Percentage of the Rep:  0.14948453608247445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Connect the test video from the device\n",
    "sample_video = cv2.VideoCapture('datasets/test_video.mp4')\n",
    "\n",
    "down = None\n",
    "counter = 0\n",
    "rep_list = []\n",
    "frame_list = []\n",
    "\n",
    "#Load the holistic model\n",
    "with holistic_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #Loop through each frame of the video \n",
    "    while sample_video.isOpened():\n",
    "        #Returns the status of the read and the frame as an image\n",
    "        status, frame = sample_video.read()\n",
    "        \n",
    "        #If frame is read correctly, status is true\n",
    "        if status == False:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "          \n",
    "        #Recolor the captured frame from BGR to RGB (Medipipe requies frames to be in RGB format)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Prevent writing and copying frame data to improve performance while making the detection\n",
    "        rgb_frame.flags.writeable = False        \n",
    "        \n",
    "        #Use holistic model to make detections\n",
    "        result_frame = holistic.process(rgb_frame)\n",
    "        \n",
    "        #Set frame back to writable format after detection\n",
    "        rgb_frame.flags.writeable = True   \n",
    "        \n",
    "        #Recolor the captured frame from BGR for rendering with opencv\n",
    "        bgr_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #Predict the coordinates of the landmarks (resulrs screen)\n",
    "        try:\n",
    "            #Extracting all the landmarks of the pose as an array\n",
    "            pose_landmarks_array = result_frame.pose_landmarks.landmark\n",
    "            \n",
    "            #Format landmarks in to a numpy array for better structuring(removing keys) and collapse array to 1 dimesnsion\n",
    "            pose_landmarks_nparray = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks_array]).flatten()\n",
    "\n",
    "            #Pass the numpy array into a data frame\n",
    "            features = pd.DataFrame([pose_landmarks_nparray])\n",
    "            \n",
    "            #Store the top class of the prediction\n",
    "            pose_class_status = model.predict(features.values)[0]\n",
    "            #Store the probability of the prediction\n",
    "            pose_class_status_prob = model.predict_proba(features.values)[0]    \n",
    "            \n",
    "            #Append correct-incorrect probabilitiesto the frame_list\n",
    "            frame_list.append(pose_class_status_prob)\n",
    "            \n",
    "            #Dictionary to store the coords in pixels of the landmarks\n",
    "            points = {}\n",
    "            for id, landmark in enumerate(result_frame.pose_landmarks.landmark):\n",
    "                height, width, center = bgr_frame.shape\n",
    "                cx, cy = int(landmark.x * width), int(landmark.y * height)\n",
    "                points[id] = (cx, cy)\n",
    "\n",
    "            cv2.circle(bgr_frame, points[12], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[14], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[16], 15, (255,0,0), cv2.FILLED)\n",
    "            \n",
    "            cv2.circle(bgr_frame, points[11], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[13], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[15], 15, (255,0,0), cv2.FILLED)            \n",
    "        \n",
    "            #For Right Arm\n",
    "            if calculate_pose_angle(points[16], points[14], points[12]) >= 158: \n",
    "                down = True\n",
    "            \n",
    "            #For Left Arm\n",
    "            elif calculate_pose_angle(points[15], points[13], points[11]) >= 158:\n",
    "                down = True\n",
    "       \n",
    "            #For Right Arm\n",
    "            if down and calculate_pose_angle(points[16], points[14], points[12]) <= 90: \n",
    "                down = False\n",
    "                counter += 1\n",
    "                rep_list.append(frame_list)\n",
    "                frame_list = []\n",
    "            \n",
    "            #For Left Arm\n",
    "            elif down and calculate_pose_angle(points[15], points[13], points[11]) <= 90:\n",
    "                down = False\n",
    "                counter += 1\n",
    "                rep_list.append(frame_list)\n",
    "                frame_list = []\n",
    "            \n",
    "            #Set a rectangle box to display the results of the prediction in the video frame\n",
    "            #rectangle(container, top_coord, bottom_coord, color, line_thickness)\n",
    "            cv2.rectangle(bgr_frame, (0,0), (600, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            #Display the class label inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Class'\n",
    "                        , (10,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract =and display the top class of the prediction\n",
    "            cv2.putText(bgr_frame, pose_class_status.split(' ')[0]\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            #Display the class probability inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Probability'\n",
    "                        , (250,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract and dispthe maximum probability\n",
    "            cv2.putText(bgr_frame, str(round(pose_class_status_prob[np.argmax(pose_class_status_prob)],2))\n",
    "                        , (250,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            #Display the class probability inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Counter'\n",
    "                        , (450,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract and dispthe maximum probability\n",
    "            cv2.putText(bgr_frame, str(counter)\n",
    "                        , (450,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    " \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #Display the frames    \n",
    "        cv2.imshow('Results Feed', bgr_frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "#Display the overall result of the exercise            \n",
    "for i in range(counter):\n",
    "    x_total = 0\n",
    "    y_total = 0\n",
    "    iter = 0\n",
    "    for list in rep_list[i]:\n",
    "        x_total += list[0]\n",
    "        y_total += list[1]\n",
    "        iter += 1\n",
    "    x_avg = x_total / iter\n",
    "    y_avg = y_total / iter\n",
    "    \n",
    "    if (x_avg >= 0.7):\n",
    "        feedback = \"Correct\"\n",
    "    else:\n",
    "        feedback == \"Incorrect\"\n",
    "    \n",
    "    print(\"The Rep No \", i + 1, \" was \", feedback)\n",
    "    print(\"Average Correct Percentage of the Rep: \", x_avg)\n",
    "    print(\"Average Incorrect Percentage of the Rep: \", y_avg)\n",
    "    print()\n",
    "\n",
    "sample_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a199944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
